## Application 3 (pr√©paration) {.smaller}

:::{.callout-tip .nonincremental collapse="true" icon=false}
# Partie 0: pr√©paration de l'exercice
* Remplacer le contenu du script `download_data` en copiant-collant le contenu de [ce fichier](https://raw.githubusercontent.com/InseeFrLab/formation-bonnes-pratiques-exo-correction/refs/heads/dev/R/download_data.R). Ex√©cuter ce script, il cr√©e les fichiers n√©cessaires pour ces exercices.
* Cr√©er le script `R/benchmarking_functions.R` en copiant-collant le contenu de [ce fichier](https://raw.githubusercontent.com/InseeFrLab/formation-bonnes-pratiques-exo-correction/refs/heads/dev/R/benchmark_functions.R)
* Cr√©er un nouveau script `R` qui servira de bac √† sable pour tester le format `Parquet`.
* Cr√©er les variables qui seront utiles pour les prochaines questions

```{.r}
columns_subset <- c(
  "REGION", "AGED", "ANAI", "CATL", "COUPLE",
  "SEXE", "SURF", "TP", "TRANS"
)

filename_sample_csv <- "data/RPindividus_24.csv"
filename_sample_parquet <- gsub("csv", "parquet", filename_sample_csv)
filename_full_parquet <- gsub("_24", "", filename_sample_parquet)
filename_full_csv <- gsub("parquet", "csv", filename_full_parquet)
```

:::

## Application 3 (partie 1) {.smaller}

::: {.callout-tip .nonincremental collapse="true" icon=false}
## Partie 1: Ouvrir un fichier `Parquet` et comprendre la logique de la lecture par bloc

Lecture du fichier avec `read_parquet` du _package_ `arrow`:

* Lire les donn√©es dont le chemin est stock√© dans `filename_sample_parquet`. Pour mesurer le temps d'ex√©cution, vous pouvez utiliser le squelette de code sugg√©r√© ci-dessous üëáÔ∏è.
* Faire la m√™me chose mais cette fois, ajouter un filtre _ex post_ avec les colonnes (`select(any_of(columns_subset))`). Mesurez-vous une diff√©rence dans les temps de traitement ?

Lecture du fichier avec `open_dataset` du _package_ `arrow`:

* Cette fois, lire le fichier avec `open_dataset(filename_sample_parquet)`. Regarder la classe de cet objet. 
* Faire un `head(5)` apr√®s `open_dataset`. Observer l'objet obtenu (sortie en console, classe).
* Maintenant regarder lorsque vous ajouter `collect()` apr√®s cette cha√Æne.
* Mesurer le temps d'ex√©cution de `open_dataset(filename_sample_parquet) %>% collect()`. Ajouter le filtre `select(any_of(columns_subset))`. Sa place influence-t-elle la vitesse de votre processus ? 

Comparaison √† la lecture d'un CSV: 

* Utiliser `readr::read_csv` pour lire le fichier (chemin `filename_sample_csv`) avec et sans l'argument `col_select`. Avez-vous des gains de performance si vous ne lisez le fichier qu'avec ces colonnes ? 

<details>

<summary>
Mesurer le temps d'ex√©cution
</summary>

```{.r}
start_time <- Sys.time()
# lecture du fichier ici
end_time <- Sys.time()
diff_time <- end_time - start_time
```

</details>

:::

_‚ùìÔ∏è Quelle m√©thode retenir pour lire un `Parquet` avec `Arrow` ?_

## Application 3 (partie 2) {.smaller}

::: {.callout-tip .nonincremental collapse="true" icon=false}
## Partie 2: Un format l√©ger et efficace

Dans cet exercice, vous devrez utiliser `open_dataset` pour lire les `Parquet`. 

* Observer l'espace disque de chaque fichier par le biais de l'explorateur de fichiers
* Mesurer le temps d'ex√©cution de la lecture du fichier dont le chemin est stock√© dans la variable `filename_full_parquet`. 
    + Faire ceci avec et sans le filtre des colonnes[^csv].
    + La croissance du temps de traitement vous appara√Æt-elle √©norme ? 
* Ajouter apr√®s cette √©tape de lecture du fichier le filter `filter(REGION == "24")`. Comprenez-vous pourquoi vous ne b√©n√©ficiez pas de gain de performance ?   

:::

_‚ùìÔ∏è Dans quel ordre sont faits les filtres par `Arrow` ?_

[^csv]: Ne pas faire ceci maintenant avec le CSV, le _benchmark_ arrive prochainement.

## Application 3 (partie 2) {.smaller}

::: {.callout-tip .nonincremental collapse="true" icon=false}
## Partie 2bis (optionnelle): Pour mieux comprendre le _predicate pushdown_ (optionnel)

`duckdb` fournit une m√©thode `EXPLAIN ANALYZE` pratique pour comprendre les optimisations faites √† la lecture d'un fichier `Parquet`.

* Prendre le contenu de [ce script](https://raw.githubusercontent.com/InseeFrLab/formation-bonnes-pratiques-exo-correction/refs/heads/dev/R/benchmark_optional.R)
* Ex√©cuter les diff√©rentes requ√™tes et regarder les diagrammes obtenus. 
* Comprenez-vous l'ordre du plan d'ex√©cution et l'effet sur les besoins de `R` ?

:::

_‚ùìÔ∏è Dans quel ordre sont faits les filtres par `Arrow` ? Comment faire si on veut r√©guli√®rement filtrer nos donn√©es √† partir de niveaux des variables ?_

## Application 3 (partie 3) {.smaller}

::: {.callout-tip .nonincremental collapse="true" icon=false}
# Partie 3: le Parquet partitionn√©

* Utiliser le code ci-dessous pour partitionner le fichier `Parquet` par _"REGION"_ et _"DEPT"_

```{.r}
open_dataset(filename_full_parquet) %>%
  group_by(REGION, DEPT) %>%
  write_dataset("./data/RPindividus")
```

* Observer l'arborescence de fichiers
* Utiliser `Arrow` pour lire les donn√©es de la Corse du Sud (code r√©gion 94, code d√©partement 2A) √† partir de ce fichier partitionn√©

:::

::: {.nonincremental}

_‚ùìÔ∏è Imaginons que les utilisateurs voudraient aussi se restreindre √† certains types de m√©nages en fonction de caract√©ristiques:_

* _Que faudrait-il faire ?_
* _Quelle est la limite ?_

:::

## Application 3 (partie 4) {.smaller}

:::{.callout-tip .nonincremental collapse="true" icon=false}
# Partie 4: G√©n√©ralisation des tests de performance (optionnel)

* Reprendre le contenu de [ce fichier](https://raw.githubusercontent.com/InseeFrLab/formation-bonnes-pratiques-exo-correction/refs/heads/dev/R/benchmark.R). 
    * Installer les packages n√©cessaires (ils sont list√©s au d√©but du script)
    * Ex√©cuter le script. Pendant que l'import en CSV tourne (c'est assez long), explorer les fen√™tres interactives qui se sont ouvertes issues du _profiling_ permettant de mesurer la r√©partition du temps de diff√©rents processus.
* Quand tout a fini de tourner, regarder le tableau obtenu.
:::


![](img/tableau-perf-parquet.png){fig-align="center"}


## Application 3 (partie 4) {.smaller}

:::{.callout-tip .nonincremental collapse="true" icon=false}
# Partie 5: mise √† jour de la cha√Æne de production

* Lire les donn√©es √† partir du morceau de code propos√©
* Changer la partie mod√©lisation pour ne pas faire tourner celle-ci sur un gros mod√®le (proposition ci-dessous)
* V√©rifier que le code tourne de A √† Z et changer celui-ci marginalement si ce n'est pas le cas

<details>

<summary>
Modification du code pour l'import de donn√©es
</summary>

```{.r}
columns_subset <- c(
  "REGION", "AGED", "ANAI", "CATL", "COUPLE",
  "SEXE", "SURF", "TP", "TRANS"
)

df <- open_dataset(
  "./data/RPindividus",
  hive_style = TRUE
) %>%
  filter(REGION == 24) %>%
  select(any_of(columns_subset)) %>%
  collect()

df <- df %>%
  rename_with(tolower) %>%
  as_tibble()
```

</details>


<details>

<summary>
Changement propos√© pour la partie mod√©lisation
</summary>


```{.r}
echantillon_modelisation <- df %>%
  filter(surf != "Z") %>%
  sample_n(1000)

MASS::polr(surf ~ couple + sexe, echantillon_modelisation)
```

</details>

:::



## Checkpoint

::: {.callout-caution}
## Checkpoint

En ligne de commande:

```{.bash}
git stash
git checkout appli3
```

![](checkpoint.jpg){width="30%"}

:::